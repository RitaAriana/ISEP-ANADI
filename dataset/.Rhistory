resNeigh$accuracy == max(resNeigh$accuracy),
'orangered1',
'steelblue4'
)
)
cvf <- 10
folds <- sample(1:cvf, nrow(datasetEx1), replace = TRUE)
#Fold size
table(folds)
k=k[which.max(accuracy)]
accuracy <- matrix(nrow = cvf, ncol = 2)
ncols <- dim(datasetEx1)[2]
for (i in 1:cvf){
train.cv <- datasetEx1.norm[folds != i, ]
test.cv <- datasetEx1.norm[folds == i, ]
train_labels <- datasetEx1[folds != i, "ProLevel"]
tst_labels <- datasetEx1[folds == i, "ProLevel"]
knn.pred <- knn(train=train.cv[,-ncols], test=test.cv[,-ncols], cl=train_labels, k)
cfmatknn <- table(tst_labels,knn.pred)
rpart.model <- rpart(ProLevel ~ ., method="class", data=train.cv)
rpart.pred <- predict(rpart.model, test.cv, type = "class")
cfmatrpart <- table(tst_labels,rpart.pred)
accuracy[i, ] <- c( sum(diag(cfmatknn))/sum(cfmatknn),
sum(diag(cfmatrpart))/sum(cfmatrpart))
}
accuracy
apply(accuracy,2,mean)
apply(accuracy,2,sd)
k[which.max(accuracy)]
k <- c()
accuracy <- c()
for (i in seq(1, 50, 2)){
knn.pred <- knn(train = datasetEx1.norm.train,
test = datasetEx1.norm.test,
cl = train_labels, k=i)
cfmatrix <- table(test_labels, knn.pred)
accuracy <- c(accuracy, sum(diag(cfmatrix))/sum(cfmatrix))
k <- c(k,i)
}
datasetEx1.norm <- as.data.frame(lapply(datasetEx1, normalise))
colnames(datasetEx1.norm) <- c( 'GroupA', 'GroupB', 'GroupC', 'GroupD', 'GroupE', 'Cobblestones', 'Hill',
'Mountain','NoneBackground','Sprinter','TimeTrial','Africa','Asia','Australia',
'Europe','NorthAmerica','SouthAmerica','altitude_results','vo2_results', 'hr_results', 'age', 'gender','ProLevel',
'WinterTrainingCamp')
datasetEx1.norm.train <- datasetEx1.norm[sample,]
datasetEx1.norm.test <- datasetEx1.norm[!sample,]
train_labels <- datasetEx1.norm[sample, "ProLevel"]
test_labels <- datasetEx1.norm[!sample, "ProLevel"]
#-------------------------------
# 1 internal node
# numnodes <- 1
#-------------------------------
# 2 internal nodes
# numnodes <- 2
#-------------------------------
# 3 internal nodes
# numnodes <- 3
#-------------------------------
# 2 internal levels: 3,2 nodes
numnodes <- c(3, 2)
nn.model <-
neuralnet(
ProLevel ~ gender + vo2_results + hr_results + age + altitude_results + WinterTrainingCamp + GroupA + GroupB + GroupC + GroupD + GroupE + Cobblestones + Hill + Mountain + NoneBackground + Sprinter + TimeTrial + Africa + Asia + Australia + Europe + NorthAmerica + SouthAmerica,
data = datasetEx1.norm.train,
hidden = numnodes,
stepmax=1e7
)
plot(nn.model)
nn.pred <- predict(nn.model, datasetEx1.norm.test)
nn.pred <- ifelse(nn.pred > 0.5, "1", "0")
accuracy_nn <- sum(nn.pred == datasetEx1.norm.test$ProLevel) / length(datasetEx1.norm.test$ProLevel) * 100
m.conf <- table(datasetEx1.norm.test$ProLevel, nn.pred)
parse_results(m.conf)
# KNN
k <- c()
accuracy <- c()
for (i in seq(1, 50, 2)){
knn.pred <- knn(train = datasetEx1.norm.train,
test = datasetEx1.norm.test,
cl = train_labels, k=i)
cfmatrix <- table(test_labels, knn.pred)
accuracy <- c(accuracy, sum(diag(cfmatrix))/sum(cfmatrix))
k <- c(k,i)
}
resNeigh <- data.frame(k, accuracy)
resNeigh[resNeigh$accuracy == max(resNeigh$accuracy), ]
plot(resNeigh$k, resNeigh$accuracy)
k[which.max(accuracy)]
plot(
resNeigh$k,
resNeigh$accuracy,
col = ifelse(
resNeigh$accuracy == max(resNeigh$accuracy),
'orangered1',
'steelblue4'
)
)
model_knn <- knn(train = datasetEx1.norm.train,
test = datasetEx1.norm.test,
cl = train_labels,
k = k[which.max(accuracy)]
)
# Confusion Matrix
m.conf <- table(datasetEx1.norm.test$ProLevel, model_knn)
parse_results(m.conf)
model_knn <- knn(train = datasetEx1.norm.train,
test = datasetEx1.norm.test,
cl = train_labels,
k = k[which.max(accuracy)]
)
# Confusion Matrix
m.conf <- table(datasetEx1.norm.test$ProLevel, model_knn)
parse_results(m.conf)
k[which.max(accuracy)]
plot(
resNeigh$k,
resNeigh$accuracy,
col = ifelse(
resNeigh$accuracy == max(resNeigh$accuracy),
'orangered1',
'steelblue4'
)
)
# capacidade preditiva relativamente ao atributo “WinterTrainingCamp”
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 2
datasetEx2 <- ciclismo_encoded
datasetEx2.norm <- as.data.frame(lapply(datasetEx2, normalise))
colnames(datasetEx2.norm) <- c( 'GroupA', 'GroupB', 'GroupC', 'GroupD', 'GroupE', 'Cobblestones', 'Hill',
'Mountain','NoneBackground','Sprinter','TimeTrial','Africa','Asia','Australia',
'Europe','NorthAmerica','SouthAmerica','altitude_results','vo2_results', 'hr_results', 'age', 'gender','ProLevel',
'WinterTrainingCamp')
sample <- sample(c(TRUE, FALSE), nrow(datasetEx2.norm), replace=TRUE, prob=c(0.7,0.3))
datasetEx2.train <- datasetEx2.norm[sample,]
datasetEx2.test <- datasetEx2.norm[!sample,]
# Árvore de Decisão  - com todos os atributos
tree.model = rpart(WinterTrainingCamp ~ . , data = datasetEx2.train, method = "class")
rpart.plot(
tree.model,
digits = 3,
fallen.leaves = TRUE,
type = 3
)
rpart.plot(tree.model, digits=3, cex=0.85, main="Árvore de Decisão")
tree.pred = predict(tree.model, datasetEx2.test, type = 'class')
#confusionMatrix
m.conf <- table(datasetEx2.test$WinterTrainingCamp, tree.pred)
parse_results(m.conf)
# Rede Neuronal
#-------------------------------
# 1 internal node
numnodes <- 1
#-------------------------------
# 2 internal nodes
# numnodes <- 2
#-------------------------------
# 3 internal nodes
# numnodes <- 3
#-------------------------------
# 2 internal levels: 3,2 nodes
# numnodes <- c(3, 2)
nn.model <-
neuralnet(
WinterTrainingCamp ~ gender + vo2_results + hr_results + age + altitude_results + ProLevel + GroupA + GroupB + GroupC + GroupD + GroupE + Cobblestones + Hill + Mountain + NoneBackground + Sprinter + TimeTrial + Africa + Asia + Australia + Europe + NorthAmerica + SouthAmerica,
data = datasetEx2.train,
hidden = numnodes,
stepmax=1e7
)
plot(nn.model)
nn.pred <- predict(nn.model, datasetEx2.test)
nn.pred <- ifelse(nn.pred > 0.5, "1", "0")
m.conf <- table(datasetEx2.test$WinterTrainingCamp, nn.pred)
parse_results(m.conf)
# capacidade preditiva relativamente ao atributo “Pro_level”
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 1
datasetEx1 <- ciclismo_encoded
datasetEx1.norm <- as.data.frame(lapply(datasetEx1, normalise))
sample <- sample(c(TRUE, FALSE), nrow(datasetEx1), replace=TRUE, prob=c(0.7,0.3))
datasetEx1.train <- datasetEx1[sample,]
datasetEx1.test <- datasetEx1[!sample,]
# Árvore de Decisão  - com todos os atributos
tree.model = rpart(ProLevel ~ . , data = datasetEx1.train, method = "class")
rpart.plot(
tree.model,
digits = 3,
fallen.leaves = TRUE,
type = 3
)
rpart.plot(tree.model, digits=3, cex=0.85, main="Árvore de Decisão")
tree.pred = predict(tree.model, datasetEx1.test, type = 'class')
#confusionMatrix
m.conf <- table(datasetEx1.test$ProLevel, tree.pred)
parse_results(m.conf)
# capacidade preditiva relativamente ao atributo “Pro_level”
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 1
datasetEx1 <- ciclismo_encoded
datasetEx1.norm <- as.data.frame(lapply(datasetEx1, normalise))
sample <- sample(c(TRUE, FALSE), nrow(datasetEx1.norm), replace=TRUE, prob=c(0.7,0.3))
datasetEx1.train <- datasetEx1.norm[sample,]
datasetEx1.test <- datasetEx1.norm[!sample,]
# Árvore de Decisão  - com todos os atributos
tree.model = rpart(ProLevel ~ . , data = datasetEx1.train, method = "class")
rpart.plot(
tree.model,
digits = 3,
fallen.leaves = TRUE,
type = 3
)
rpart.plot(tree.model, digits=3, cex=0.85, main="Árvore de Decisão")
tree.pred = predict(tree.model, datasetEx1.test, type = 'class')
#confusionMatrix
m.conf <- table(datasetEx1.test$ProLevel, tree.pred)
parse_results(m.conf)
datasetEx1.norm <- as.data.frame(lapply(datasetEx1, normalise))
colnames(datasetEx1.norm) <- c( 'GroupA', 'GroupB', 'GroupC', 'GroupD', 'GroupE', 'Cobblestones', 'Hill',
'Mountain','NoneBackground','Sprinter','TimeTrial','Africa','Asia','Australia',
'Europe','NorthAmerica','SouthAmerica','altitude_results','vo2_results', 'hr_results', 'age', 'gender','ProLevel',
'WinterTrainingCamp')
datasetEx1.norm.train <- datasetEx1.norm[sample,]
datasetEx1.norm.test <- datasetEx1.norm[!sample,]
train_labels <- datasetEx1.norm[sample, "ProLevel"]
test_labels <- datasetEx1.norm[!sample, "ProLevel"]
model_knn <- knn(train = datasetEx1.norm.train,
test = datasetEx1.norm.test,
cl = train_labels,
k = k[which.max(accuracy)]
)
# Confusion Matrix
m.conf <- table(datasetEx1.norm.test$ProLevel, model_knn)
parse_results(m.conf)
cvf <- 10
folds <- sample(1:cvf, nrow(datasetEx1), replace = TRUE)
#Fold size
table(folds)
k=k[which.max(accuracy)]
accuracy <- matrix(nrow = cvf, ncol = 2)
ncols <- dim(datasetEx1)[2]
for (i in 1:cvf){
train.cv <- datasetEx1.norm[folds != i, ]
test.cv <- datasetEx1.norm[folds == i, ]
train_labels <- datasetEx1[folds != i, "ProLevel"]
tst_labels <- datasetEx1[folds == i, "ProLevel"]
knn.pred <- knn(train=train.cv[,-ncols], test=test.cv[,-ncols], cl=train_labels, k)
cfmatknn <- table(tst_labels,knn.pred)
rpart.model <- rpart(ProLevel ~ ., method="class", data=train.cv)
rpart.pred <- predict(rpart.model, test.cv, type = "class")
cfmatrpart <- table(tst_labels,rpart.pred)
accuracy[i, ] <- c( sum(diag(cfmatknn))/sum(cfmatknn),
sum(diag(cfmatrpart))/sum(cfmatrpart))
}
accuracy
apply(accuracy,2,mean)
apply(accuracy,2,sd)
# Existe diferença significativa no desempenho dos dois melhores modelos  obtidos  anteriormente?
# verificar se é distribuição normal
shapiro.test(accuracy[,1]-accuracy[,2])
wilcox.test(accuracy[,1], accuracy[,2], paired = TRUE)
shapiro.test(accuracy[,1]-accuracy[,2])
skewness(mlr.pred - nn.pred.vo2) #
mlr.model <- lm(vo2_results ~ .,
data = datasetEx7.train)
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 7
datasetEx7 <- ciclismo.encoded
ciclismo.encoded <- ciclismo
ciclismo.encoded$Team= as.numeric(as.factor(ciclismo.encoded$Team)) - 1
ciclismo.encoded$Background = as.numeric(as.factor(ciclismo.encoded$Background)) - 1
ciclismo.encoded$Continent = as.numeric(as.factor(ciclismo.encoded$Continent)) - 1
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 7
datasetEx7 <- ciclismo.encoded
# Dividir o dataset em 70% para treino e 30% para teste - Critério Holdout
sample <- sample(c(TRUE, FALSE), nrow(datasetEx7), replace=TRUE, prob=c(0.7,0.3))
datasetEx7.train <- datasetEx7[sample,]
datasetEx7.test <- datasetEx7[!sample,]
# Regressão linear múltipla - com todos os atributos
mlr.model <- lm(vo2_results ~ .,
data = datasetEx7.train)
summary(mlr.model)
mlr.pred <- predict(mlr.model,datasetEx7.test)
mlr.mae <- MAE(mlr.pred, datasetEx7.test$vo2_results)
mlr.rmse <- RMSE(mlr.pred, datasetEx7.test$vo2_results)
datasetEx7.norm <- as.data.frame(lapply(datasetEx7, normalise))
datasetEx7.norm.train <- datasetEx7.norm[sample,]
datasetEx7.norm.test <- datasetEx7.norm[!sample,]
#-------------------------------
# 1 internal node
numnodes <- 1
#-------------------------------
# 3 internal nodes
# numnodes <- 3
#-------------------------------
# 2 internal levels: 3,2 nodes
# numnodes <- c(3, 2)
#-------------------------------
# 2 internal levels: 6,2 nodes
# numnodes <- c(6, 2)
nn.model <-
neuralnet(
vo2_results ~ .,
data = datasetEx7.norm.train,
hidden = numnodes
)
plot(nn.model)
nn.pred <- compute(nn.model, datasetEx7.norm.test)
# Desnormalizar os dados para avaliar o modelo
nn.pred.vo2 <- minmaxdesnorm(nn.pred$net.result,datasetEx7$vo2_results)
test.vo2 <- minmaxdesnorm(datasetEx7.norm.test$vo2_results,datasetEx7$vo2_results)
nn.mae <- MAE(nn.pred.vo2, test.vo2)
nn.rmse <- RMSE(nn.pred.vo2, test.vo2)
lillie.test(mlr.pred - nn.pred.vo2) # amostra > 30
# p-value =  2.148e-12 < 0.05 - distribuição não normal
# simetria da distribuição
skewness(mlr.pred - nn.pred.vo2) #
wilcox.test(mlr.pred, nn.pred.vo2, paired = TRUE)
# p-value = 0.1729 > 0.05 , n
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 2
datasetEx2 <- ciclismo_encoded
datasetEx2.norm <- as.data.frame(lapply(datasetEx2, normalise))
colnames(datasetEx2.norm) <- c( 'GroupA', 'GroupB', 'GroupC', 'GroupD', 'GroupE', 'Cobblestones', 'Hill',
'Mountain','NoneBackground','Sprinter','TimeTrial','Africa','Asia','Australia',
'Europe','NorthAmerica','SouthAmerica','altitude_results','vo2_results', 'hr_results', 'age', 'gender','ProLevel',
'WinterTrainingCamp')
sample <- sample(c(TRUE, FALSE), nrow(datasetEx2.norm), replace=TRUE, prob=c(0.7,0.3))
datasetEx2.train <- datasetEx2.norm[sample,]
datasetEx2.test <- datasetEx2.norm[!sample,]
# Árvore de Decisão  - com todos os atributos
tree.model = rpart(WinterTrainingCamp ~ . , data = datasetEx2.train, method = "class")
rpart.plot(
tree.model,
digits = 3,
fallen.leaves = TRUE,
type = 3
)
rpart.plot(tree.model, digits=3, cex=0.85, main="Árvore de Decisão")
tree.pred = predict(tree.model, datasetEx2.test, type = 'class')
#confusionMatrix
m.conf <- table(datasetEx2.test$WinterTrainingCamp, tree.pred)
parse_results(m.conf)
# capacidade preditiva relativamente ao atributo “WinterTrainingCamp”
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 2
datasetEx2 <- ciclismo_encoded
datasetEx2.norm <- as.data.frame(lapply(datasetEx2, normalise))
colnames(datasetEx2.norm) <- c( 'GroupA', 'GroupB', 'GroupC', 'GroupD', 'GroupE', 'Cobblestones', 'Hill',
'Mountain','NoneBackground','Sprinter','TimeTrial','Africa','Asia','Australia',
'Europe','NorthAmerica','SouthAmerica','altitude_results','vo2_results', 'hr_results', 'age', 'gender','ProLevel',
'WinterTrainingCamp')
sample <- sample(c(TRUE, FALSE), nrow(datasetEx2), replace=TRUE, prob=c(0.7,0.3))
datasetEx2.train <- datasetEx2[sample,]
datasetEx2.test <- datasetEx2[!sample,]
# Árvore de Decisão  - com todos os atributos
tree.model = rpart(WinterTrainingCamp ~ . , data = datasetEx2.train, method = "class")
rpart.plot(
tree.model,
digits = 3,
fallen.leaves = TRUE,
type = 3
)
rpart.plot(tree.model, digits=3, cex=0.85, main="Árvore de Decisão")
tree.pred = predict(tree.model, datasetEx2.test, type = 'class')
#confusionMatrix
m.conf <- table(datasetEx2.test$WinterTrainingCamp, tree.pred)
parse_results(m.conf)
# capacidade preditiva relativamente ao atributo “WinterTrainingCamp”
# garantir a reprodutibilidade dos resultados
set.seed(123)
# dataset exercício 2
datasetEx2 <- ciclismo_encoded
datasetEx2.norm <- as.data.frame(lapply(datasetEx2, normalise))
colnames(datasetEx2.norm) <- c( 'GroupA', 'GroupB', 'GroupC', 'GroupD', 'GroupE', 'Cobblestones', 'Hill',
'Mountain','NoneBackground','Sprinter','TimeTrial','Africa','Asia','Australia',
'Europe','NorthAmerica','SouthAmerica','altitude_results','vo2_results', 'hr_results', 'age', 'gender','ProLevel',
'WinterTrainingCamp')
sample <- sample(c(TRUE, FALSE), nrow(datasetEx2.norm), replace=TRUE, prob=c(0.7,0.3))
datasetEx2.train <- datasetEx2.norm[sample,]
datasetEx2.test <- datasetEx2.norm[!sample,]
# Rede Neuronal
#-------------------------------
# 1 internal node
numnodes <- 1
#-------------------------------
# 2 internal nodes
# numnodes <- 2
#-------------------------------
# 3 internal nodes
# numnodes <- 3
#-------------------------------
# 2 internal levels: 3,2 nodes
# numnodes <- c(3, 2)
nn.model <-
neuralnet(
WinterTrainingCamp ~ gender + vo2_results + hr_results + age + altitude_results + ProLevel + GroupA + GroupB + GroupC + GroupD + GroupE + Cobblestones + Hill + Mountain + NoneBackground + Sprinter + TimeTrial + Africa + Asia + Australia + Europe + NorthAmerica + SouthAmerica,
data = datasetEx2.train,
hidden = numnodes,
stepmax=1e7
)
plot(nn.model)
nn.pred <- predict(nn.model, datasetEx2.test)
nn.pred <- ifelse(nn.pred > 0.5, "1", "0")
m.conf <- table(datasetEx2.test$WinterTrainingCamp, nn.pred)
parse_results(m.conf)
cvf <- 10
folds <- sample(1:cvf, nrow(datasetEx2), replace = TRUE)
#Fold size
table(folds)
accuracy <- matrix(nrow = cvf, ncol = 2)
ncols <- dim(datasetEx2)[2]
for (i in 1:cvf){
train.cv <- datasetEx2.norm[folds != i, ]
test.cv <- datasetEx2.norm[folds == i, ]
train_labels <- datasetEx2[folds != i, "WinterTrainingCamp"]
tst_labels <- datasetEx2[folds == i, "WinterTrainingCamp"]
# Rede Neuronal
nn.model <-
neuralnet(
WinterTrainingCamp ~ gender + vo2_results + hr_results + age + altitude_results + ProLevel + GroupA + GroupB + GroupC + GroupD + GroupE + Cobblestones + Hill + Mountain + NoneBackground + Sprinter + TimeTrial + Africa + Asia + Australia + Europe + NorthAmerica + SouthAmerica,
data = train.cv,
hidden = numnodes,
stepmax=1e7
)
# Fazer as previsões através do modelo de rede neuronal
predictions_nn <- predict(nn.model,test.cv)
predictions_nn <- ifelse(predictions_nn > 0.5, "1", "0")
# Matriz de confusão
cfmatknn <- table(tst_labels,predictions_nn)
# Árvore de Decisão
rpart.model <- rpart(WinterTrainingCamp ~ ., method="class", data=train.cv)
rpart.pred <- predict(rpart.model, test.cv, type = "class")
cfmatrpart <- table(tst_labels,rpart.pred)
accuracy[i, ] <- c( sum(diag(cfmatknn))/sum(cfmatknn),
sum(diag(cfmatrpart))/sum(cfmatrpart))
}
accuracy
apply(accuracy,2,mean)
apply(accuracy,2,sd)
shapiro.test(accuracy[,1]-accuracy[,2]) # p-value =  0.02661, existe normalidade
t.test<-t.test(accuracy[,1], accuracy[,2], paired = TRUE)
p_value <- t.test$p.value # p-value < alpha
shapiro.test(accuracy[,1]-accuracy[,2]) # p-value =  0.02661, existe normalidade
t.test<-t.test(accuracy[,1], accuracy[,2], paired = TRUE)
p_value <- t.test$p.value # p-value < alpha
p_value
t.test<-t.test(accuracy[,1], accuracy[,2], paired = TRUE)
p_value
# Bibliotecas necessárias
library(readr)
library(corrplot)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(neuralnet)
library(Metrics)
library(caret)
library(class)
library(FNN)
library(nortest)
library(moments)
# Cores para os gráficos
PaleBlue<- rgb(171,196,255, maxColorValue = 255)
Blue2<- rgb(136,143,205, maxColorValue = 255)
Blue3<- rgb(98,109,167, maxColorValue = 255)
Blue4<- rgb(61,74,129, maxColorValue = 255)
# Função para calcular o RMSE
RMSE <- function(test, predicted) {
sqrt(mean((test - predicted) ^ 2))
}
# Função para calcular o MAE
MAE <- function(test, predicted) {
mean(abs(test - predicted))
}
# Carregar o ficheiro com os dados (ciclismo.csv) - Rita Ariana (1201386)
setwd("/home/rita/Desktop/dev/anadi/anadi_pdn_3de3df_1201518_1201386_1202016/dataset")
ciclismo <- read.csv("ciclismo.csv", header = FALSE)
# Definir os nomes das colunas
colnames(ciclismo) <- c( 'ID', 'gender', 'Team', 'Background', 'ProLevel', 'WinterTrainingCamp', 'altitude_results',
'vo2_results', 'hr_results', 'dob', 'Continent')
# Remover a primeira linha (cabeçalho)
ciclismo <- ciclismo[-1,]
######################################### Análise dos Dados #########################################
# Quantidade de linhas e colunas do dataset
# dim(ciclismo)
nrow(ciclismo)
ncol(ciclismo)
# Obter um sumário dos dados - análise descritiva
summary(ciclismo)
str(ciclismo)
# Derivar o atributo Age a partir do atributo dob (ex: 1988-01-09 - YYYY-MM-DD)
ciclismo$age <- as.numeric(format(Sys.Date(), "%Y")) - as.numeric(format(as.Date(ciclismo$dob), "%Y")) -
(as.numeric(format(Sys.Date(), "%m%d")) < as.numeric(format(as.Date(ciclismo$dob), "%m%d")))
# Transformar os atributos altitude_results, vo2_results e hr_results em numéricos
ciclismo$altitude_results <- as.numeric(ciclismo$altitude_results)
ciclismo$vo2_results <- as.numeric(ciclismo$vo2_results)
ciclismo$hr_results <- as.numeric(ciclismo$hr_results)
plot(ciclismo$altitude_results, ciclismo$vo2_results, main = "altitude_results vs vo2_results", xlab = "altitude_results", ylab = "vo2_results")
plot(ciclismo$altitude_results, ciclismo$hr_results, main = "altitude_results vs hr_results", xlab = "altitude_results", ylab = "hr_results")
plot(ciclismo$altitude_results, ciclismo$age, main = "altitude_results vs age", xlab = "altitude_results", ylab = "age")
plot(ciclismo$vo2_results, ciclismo$hr_results, main = "vo2_results vs hr_results", xlab = "vo2_results", ylab = "hr_results")
plot(ciclismo$vo2_results, ciclismo$age, main = "vo2_results vs age", xlab = "vo2_results", ylab = "age")
plot(ciclismo$hr_results, ciclismo$age, main = "hr_results vs age", xlab = "hr_results", ylab = "age")
plot(ciclismo$vo2_results, ciclismo$age, main = "vo2_results vs age", xlab = "vo2_results", ylab = "age")
plot(ciclismo$vo2_results, ciclismo$hr_results, main = "vo2_results vs hr_results", xlab = "vo2_results", ylab = "hr_results")
